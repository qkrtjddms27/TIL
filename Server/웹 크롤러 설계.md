# 웹 크롤러 설계
> 웹 크롤러는 로봇 또는 스파이더라고도 부른다. 검색 엔진에서 널리 쓰는 기술로, 웹에 새로 올라오거나 갱신된 콘텐츠를 찾아내는 것이 주된 목적.

- 검색 엔진 인덱싱 : 크롤러의 가장 보편적인 사례. 웹 페이지를 모아 검색 엔진을 위한 롤컬 인덱스를 만든다.
- 웹 아카이빙 : 나중에 사용할 목적으로 장기보관하기 위해 웹에서 정보를 모으는 절차.
- 웹 마이닝 : 인터넷에서 유용한 지식을 도출해 낼 수 있다. 유명 금융 기업들은 크롤렁를 사용해 주주총회 자료나 연차 보고서를 다운받아 기업의 핵심 사업 방향을 알아내기도 한다.
- 웹 모니터링 : 크롤러를 사용하면 인터넷에서 저작권이나 상표권이 침해되는 사례를 모니터링할 수 있다. 디지마크사는 웹 크롤러를 사용해 해적판 저자묵을 찾아내서 보고한다.

웹 크롤링은 몇 시간이면 끝낼 수 있는 작은 학습 프로젝트 수준일 수 있고, 별도의 엔지니어링 팀을 꾸려서 지속적으로 관리하고 개선해야 하는 초대형 프로젝트가 될 수도 있다.

## 1단계, 문제 이해 및 설계 범위 확정
> 웹 크롤러의 기본 알고리즘은 어렵지 않다.

- URL 집합이 입력으로 주어지면, 해당 URL들이 가리키는 모든 웹 페이지를 다운로드한다.
- 다운받은 웹 페이지에서 URL들을 추출한다.
- 추출된 URL들은 다운로드할 URL 목록에 추가하고 위의 과정을 처음부터 반복한다.

> 하지만 엄청난 규모 화작성을 갖는 웹 크롤러를 설계하는 것은 엄청나게 어려운 일이다.

- 이 크롤로의 주된 용도는 무엇인가요? 검색 엔진 인덱스 생성용인가요? 아니면 데이터 마이닝? 아니면 그 외의 다른 용도가 있나요?
  + 검색 엔진 인덱싱에 쓰일 것입니다.
- 매달 얼마나 많은 웹 페이지를 수집해야 하나요?
  + 10억 개의 웹 페이지를 수집해야 합니다.
- 새로 만들어진 웹 페이지나 수정된 웹 페이지도 고려해야 하나요?
- 수집한 웹 페이지는 저장해야 하나요?
  + 5년간 저장
- 중복된 컨텐츠는 어떻게 해야 하나요?
  + 중복된 컨텐츠를 갖는 페이지는 무시해도 됩니다.
    
### 크롤러를 구현하기 위해 고려해야 하는 사항
- 규모 확장성 : 수십억 개의 페이지가 존재한다. 따라서 병행성을 활용하면 보다 효과적으로 웹 크롤링을 할 수 있다.
- 안정성 : 잘못 작성된 HTML, 아무 반응이 없는 서버, 장애, 악성 코드가 붙어 있는 링크등의 환경에서도 잘 대응할 수 있어야 한다.
- 예절 : 크롤러는 수집 대상 웹 사이트에 짧은 시간 동안 너무 많은 효청을 보내서는 안 된다.
- 확장성 : 새로운 형태의 컨텐츠를 지원하기가 쉬워야한다.

### 개략적 규모 추정
> 아래의 추정치는 많은 가정으로부터 나온 것이다. 그에 관해 합의해 두는 것이 중요하다.

- 매달 10억 개의 웹 페이지를 다운로드한다.
- QPS = 10억
- 최대 QPS = 2 * QPS = 800
- 웹 페이지의 크기 평균은 500k라고 가정
- 10억 페이지 * 500k = 500TB/월.
- 1개우러치 데이터를 보관하는 데는 500TB, 5년간 보관한다고 가정하면 결국 500TB * (12 * 5) = 30PB의 저장용량이 필요

## 개략적인 설계안 제시 및 동의 구하기

- 시작 URL 집합
- 미수집 URL 저장소
- HTML 다운로더
- 컨텐츠 파서
- 중복 컨텐츠
- URL 추출기
- URL 필터
- 이미 방문한 URL일 경우
  

### 시작 URL 집합
> 시작 URL 집합은 웹 크롤러가 크롤링을 시작하는 출발점이다.

- 어떤 대학 웹사이트로부터 찾아 나갈 수 있는 모든 웹 페이지를 크롤링하는 가장 직관적인 방법은 
  해당 대학의 도메인 이름이 붘은 모든 페이지의 URL을 시작 URI로 사용하는 것.
  
- 일반적으로 전체 URL 공간을 작은 부분집합으로 나누는 전략을 쓴다.

### 미수집 URL 저장소
> 대부분의 현대적 웹 크롤러는 크롤링 상태를 다운로드할 URL, 다운로드된 URL 두 가지로 나눠 관리한다. 
> 이 중 '다운로드할 URL'을 저장 관리하는 컴포넌트를 미수집 URL 저장소라고 부른다.

- FIFO 큐라고 생각하면 된다.

### HTML 다운로드
> HTML 다운로더는 인터넷에서 웹 페이지를 다운로드하는 컴포넌트다. 다운로드할 페이지의 URL 미수집 URl 저장소가 제공한다.

### 도메인 이름 변환기
> 웹 페이지를 다운받으려면 URL을 IP 주소로 변환하는 절차가 필요하다. HTML 다운로더는 도메인 이름 변환기를 사용하여 URL에 대응되는 IP 주소를 알아낸다.

### 콘텐츠 파서
> 웹 페이지를 다운로드하면서 파싱과 검증 절차를 거쳐야 한다. 
> 이상한 웹 페이즈는 문제를 일으킬 수 있는데다 저장 공간만 낭비하게 되기 때문에. 독립된 컴포넌트로 만들었다.

### 중복 컨텐츠
> 웹에 공개된 연구 결과에 따르면, 29% 가량의 웹 페이지 콘텐츠는 중복이다. 따라서 같은 컨텐츠를 여러 번 저장하게 될 수 있다.

- 두 HTML 문서를 비교하는 가장 간단한 방법이 있지만 10억에 달하는 경우에는 느리고 비효율적이다
- 효과적인 방법은 웹 페이지의 해시 값을 비교하는 것이다.

### 콘텐츠 저장소
> 콘텐츠 저장소는 HTML 문서를 보관하는 시스템이다. 저장소를 구현하는 데쓰일 기술을 고를 때는 저장할 
> 데이터의 유형, 크기, 저장소 접근 빈도, 데이터의 유효 기간등을 종합적으로 고려해야한다.

- 데이터 양이 너무 많으므로 대부분의 컨텐츠는 디스크에 저장한다.
- 인기 있는 컨텐츠는 메모리에 두어 접근 지연시간을 줄인다.

### URL 추출기
> URL 추출기는 HTML 페이지를 파싱하여 링크들을 골라내는 역할을 한다.
- 상대 경로는 전부 앞에 모든 주소를 붙여 절대 경로로 변환한다.

### URL 필터
> URL 필터는 특정한 컨텐츠 타입이나 파일 확장자를 갖는 URL, 접속 시 오류가 발생하는 URL, 접근 제외 목록에 포함된 URL 등을 크롤링 대상에서 배제하는 역할을 한다.

### 이미 방문한 URL?
> 이미 방문한 적이 있는 URL인지 추적하면 같은 URL을 여러 번 처리하는 일을 방지할 수 있으므로 서버 부하를 줄이고 시스템이 무한 루프에 빠지는 일을 방지할 수 있다.

- 해당 자료 구조로는 블룸 필터나 해시 테이블이 널리 쓰인다.

### URL 저장소
> URL 저장소는 이미 방문한 URL을 보관하는 저장소다.


### 웹 크롤러 작업 흐름

- 시작 URL들을 미수집 URL 저장소에 저장한다.
- HTML 다운로더는 미수집 URL 저장소에서 URL 목록을 가져온다.
- HTML 다운로더는 도에인 이름 변환기를 사용하여 URL의 IP 주소를 알아내고, 해당 IP 주소로 접속하여 웹 페이지를 다운 받는다.
- 컨텐츠 파서는 다운된 HTML 페이지를 파싱하여 올바른 형식을 갖춘 페이지인지 검증한다.
- 컨텐츠 파싱과 검증이 끝나면 중복 컨텐츠인지 확인하는 절차를 개시한다.
- 중복 컨텐츠인지 확인하기 위해서, 해당 페이지가 이미 저장소에 있는지 본다.
  + 이미 저장소에 있는 컨텐츠인 경우에는 처리하지 않고 버린다.
  + 저장소에 없는 컨텐츠인 경우에는 저장소에 저장한 뒤 URL 추출기로 전달한다.

## 3단계, 상세설계

### BFS, DFS

- 웹은 유향 그래프다.
- 그래프 크기가 클 경우 어느 정도로 깊숙이 가게 될지 가늠하기 어렵기때문에 DFS보단 BFS를 사용.
- 크롤러가 병렬로 처리하게 된다면 위키피다아 서버는 수많은 요청으로 과부하에 걸리된다.
  + 이것을 `예의 없는 크롤러`라고 한다.
- BFS 기본적으로 URL 간에 우선 순위를 두지 않는다.
  + 하지만 페이지 순위, 사용자 트래픽의 양, 업데이트 빈도 등 여러 가지 척도에 비추어 처리 우선순위를 구별한다.
  
### 미수집 URL 저장소
> 미수집 URL 저장소를 활요하면 이런 문제를 좀 쉽게 해결할 수 있다.

- URL 저장소는 다운로드할 URL을 보관할 장소다.
- URL 저장소를 잘 활욯아면 예의를 갖춘 크롤러, URL 사이의 우선순위와 신선도를 구별하는 크롤러를 구현할 수 있다.

#### 예의
> 웹 크롤러는 수집 대상 서버로 짧은 시간 안에 너무 많은 요청을 보내는 것을 삼가해야 한다.
- 너무 많은 요청을 보내는 것은 무례한으로 간주될 수 있다.
  + 동일 웹 사이트에 대해서는 한 번에 한 페이지만 요청한다는 것.
  + 같은 웹 사이트의 페이지를 다운받는 태스클는 시간차를 두고 하면 될 것.

- 각 다운로드 스레드는 별도 FIFO 큐를 가지고 있어야한다.
  + 큐 라우터: 같은 호스트에 속한 URL은 언제나 같은 큐로 가도록 보장하는 역할을 한다.
  + 매핑 테이블 : 호스트 이름과 큐사이의 관계를 보관하는 테이블.
  + FIFO 큐: 같은 호스트에 속한 URL은 언제나 같은 큐에 보관된다.
  + 큐 선택기 : 큐 선택기는 큐들을 순회하면서 큐에서 URL을 꺼내서 해당 큐에서 나온 URL을 다운로드하도록 지정된 작업 스레드에 전달하는 역할을 한다.
  + 작업 스레드 : 작업 스레드는 전달된 URL을 다운로드하는 작업을 수행한다.작업들 사이에는 일정한 지연시간을 둘 수 있다.

#### 우선순위
> 유용성에 따라 URL의 우선순의를 나눌 때는 페이지랭크, 트래픽 양, 갱신 빈도 등 다양한 척도를 사용할 수 있을 것이다.
- 순위결정장치 : URL을 입력으로 받아 우선순위를 계산한다.
- 큐 : 우선순위별로 큐가 하나씩 할당된다. 우선순위가 높으면 선택될 확률도 올라간다.
+ 큐 선택기 : 임의 큐에서 치라할 URL을 꺼내는 역하을 담당한다. 순위가 높은 큐에서 더 자주 꺼내도록 프로그램되어 있다.

#### 예의 & 우선순위
위의 두 요구사항을 만족하기 위해서는 2개의 모듈이 존재하게 된다.
- 전면 큐 : 우선순위 결정 과정을 처리한다.
- 후면 큐 : 크롤러가 예의 바르게 동작하도록 보증한다.

#### 신선도
> 웹 페이지는 수시로 추가되고, 삭제되고, 변경된다. 데이터의 신섬함을 유지하기 위해서는 이미 다운로드한 페이지라고 해도 주기적으로 재수집할 필요가 있다.

- 웹 페이지의 변경 이력 활용
- 우선순위를 활용하여, 중요한 페이지는 좀 더 자주 재수집.

#### 미수집 URL 저장소를 위한 지속성 저장장치
- 검색 엔진을 위한 크롤러의 경우, 처리해야 하는 URL의 수는 수억 개의 달한다.
- 수억개의 URL을 모두 메모리에 보관하는 것은 안정성이나 규모 확장성 측면에서 바람직하지 않다.
- 대부분의 URL은 디스크에 두지만 IO 비용을 줄이기 위해 메모리 버퍼에 큐를 두는 것이다.

### HTML 다운로더
> HTML 다운로더는 HTTP 프로토콜을 통해 웹 페이지를 내려 받는다.

#### Robtos.txt
> 웹사이트가 크롤러와 소통하는 표준적 방법.
- 웹 사이트를 긁어 가기 전에 크롤러는 해당 파일에 나열된 규칙을 먼저 확인해야 한다.
- 해당 파일도 여러 번 다운 받는 것을 피하기 위해 따로 캐시해두는 것이 좋다.
```
https://www.amazon.com/robots.txt
```

### 성능 최적화

#### 분산 크롤링
- URL 공간은 작은 단위로 분할하여, 각 서버는 그중 일부의 다운로드를 담당하도록 한다.

- 도메인 이름 변환 결과 캐시
  + DNS 요청을 보내고 결과를 받는 동기적 특성 때문이에 DNS 요청이 처리되는 데는 보통 10ms에서 200ms가 소요된다.
  + 따라서 크론 잡 등을 돌려 주기적으로 갱신하도록 해 놓으면 성능을 효과적으로 높일 수 있다.

- 지역성
  + 크롤링 작업을 수행하는 서버를 지역별로 분산하는 방법이다.
  + 크롤링 대상 서버와 지역적으로 가까우면 다운로드 시간을 줄어들 것이다.
  + 지역성을 활용하는 전략은 크롤 서버, 캐시, 큐, 저장소 등 대부분의 컴퍼넌트에 적용 가능하다.
  
- 짧은 타임아웃
> 어떤 웹 서버는 응답이 느리거나 아예 응답하지 않는다. 이런 경우 대기 시간이 길어지면 좋지 않으므로, 최대 얼마나 기다릴지를 미리 정해두는 것이다.


### 안정성
> 최적화된 성능뿐 아니라 안정성도 다운로더 설꼐 시 중요하게 고려해야 할 부분이다.

- 안정 해시
  + 다운로더 서버들에 부하를 분산할 때 적용 가능한 기술이다.
  + 이 기술을 이요하면 다운로더 서버를 쉽게 추가하고 삭제할 수있다.
  
- 크롤링 상태 및 수집 데이터 저장
  + 장애가 발생한 경우에도 쉽게 복구할 수 있도록 크롤링 상태와 수지된 데이터를 지속적 저장장치에 기록해 두는 것이 바람직하다.
  + 저장된 데이터를 로딩하고 나면 중단되었던 크롤링을 쉽게 재시작할 수 있다.
  
- 예외 처리
  + 대규모 시스템에서 에러는 불가피할뿐 아니라 흔하게 벌어지는 일이다.
  + 예외가 발생하면 전체 시스템이 중단되는 일 없이 그 작업을 우아하게 이어나갈 수 있어야 한다.

- 데이터 검증
  + 시스템을 오류를 방지하기 위한 중요 수단 가운데 하나다.
  
### 확장성
> 진화하지 않는 시스템은 없는 법이라서, 이런 시스템을 설계할 때는 새로운 형태의 컨텐츠를 쉽게 지원할 수 있도록 신경 써야 한다.

### 문제 있는 컨텐츠 감지 및 회피

#### 중복 컨테츠
- 해시나 체크섬을 사용하면 중복 컨텐츠를 보다 쉽게 탐지할 수 있다.

#### 거미 덫
- 크롤러를 무한 루프에 빠뜨리도록 설계한 웹 페이지다.
- url 길이 제한으로 어느정도 회피할 순 있지만 수작업이 필요하다.

#### 데이터 노이즈
- 광고나 스크립트 코드, 스팸 같은 것은 가능한 제외하는 것이 좋다.

## 4단계, 마무리

### 서버측 렌더링
서버측 렌더링 페이지를 파싱하기 전에 서버 측 렌더링을 적용하면 해결할 수 있다.

### 원치 않는 페이지 필터링
저장 공간 등 크롤리에 소요되는 자원은 유한하기 때문에 스팸 방지 컴포넌트를 두어 품질이 조악하거나 스팸성인 페이지를 걸러내도록 해 두면 좋다.
